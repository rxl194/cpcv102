{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./tf_mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting ./tf_mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./tf_mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./tf_mnist\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"./tf_mnist\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "training_iters = int(1e5)\n",
    "batch_size = 128\n",
    "display_step = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_input = 28\n",
    "n_steps = 28\n",
    "n_h1 = 128\n",
    "n_label = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def BiRNN(x, weights, biases):\n",
    "    # Prepare data shape to match 'bidirectional_rnn' function requirement\n",
    "    # Current data input shape: (batch_size, n_steps, n_input)\n",
    "    # Required shape: 'n_steps' tensors list of shape(batch_size, n_input)\n",
    "    \n",
    "    # Unstack to get a list of 'n_steps' tensors of shape(batch_size, n_input)\n",
    "    x = tf.unstack(x, num=n_steps, axis=1)\n",
    "    \n",
    "    # Define lstm cells with tensorflow\n",
    "    # Forward direction cell\n",
    "    lstm_fw_cell = rnn.BasicLSTMCell(n_h1, forget_bias=1.0)\n",
    "    # Backward direction cell\n",
    "    lstm_bw_cell = rnn.BasicLSTMCell(n_h1, forget_bias=1.0)\n",
    "    \n",
    "    # Get lstm cell output:\n",
    "    try:\n",
    "        outputs, _, _ = rnn.static_bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, x,\n",
    "                                                     dtype=tf.float32)\n",
    "    except Excpetion:\n",
    "        outputs = rnn.static_birectional_rnn(lstm_fw_cell, lstm_bw_cell, x, \n",
    "                                            dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    Xtr = tf.placeholder(tf.float32, [None, n_steps, n_input])\n",
    "    Ytr = tf.placeholder(tf.float32, [None, n_label])\n",
    "    \n",
    "    weights = {\n",
    "        # Hidden layer wights = 2*n_h1, because of fw+bw cells\n",
    "        'out': tf.Variable(tf.random_normal([2*n_h1, n_label]))\n",
    "    }\n",
    "    \n",
    "    biases = {\n",
    "        'out': tf.Variable(tf.random_normal([n_label]))\n",
    "    }\n",
    "    \n",
    "    pred = BiRNN(Xtr, weights, biases)\n",
    "    \n",
    "    # Define cost and optimizer\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=Ytr))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    \n",
    "    # Evaluate model\n",
    "    correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(Ytr,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 3200, Minibatch Loss=1.3698, Training Accuracy=0.5625\n",
      "iter: 6400, Minibatch Loss=0.7079, Training Accuracy=0.7578\n",
      "iter: 9600, Minibatch Loss=0.5917, Training Accuracy=0.8438\n",
      "iter: 12800, Minibatch Loss=0.3400, Training Accuracy=0.9062\n",
      "iter: 16000, Minibatch Loss=0.4046, Training Accuracy=0.8906\n",
      "iter: 19200, Minibatch Loss=0.2292, Training Accuracy=0.9219\n",
      "iter: 22400, Minibatch Loss=0.2379, Training Accuracy=0.9141\n",
      "iter: 25600, Minibatch Loss=0.2199, Training Accuracy=0.9375\n",
      "iter: 28800, Minibatch Loss=0.1850, Training Accuracy=0.9297\n",
      "iter: 32000, Minibatch Loss=0.1298, Training Accuracy=0.9609\n",
      "iter: 35200, Minibatch Loss=0.2192, Training Accuracy=0.9141\n",
      "iter: 38400, Minibatch Loss=0.2761, Training Accuracy=0.9141\n",
      "iter: 41600, Minibatch Loss=0.2370, Training Accuracy=0.9141\n",
      "iter: 44800, Minibatch Loss=0.2119, Training Accuracy=0.9375\n",
      "iter: 48000, Minibatch Loss=0.2465, Training Accuracy=0.9062\n",
      "iter: 51200, Minibatch Loss=0.1354, Training Accuracy=0.9453\n",
      "iter: 54400, Minibatch Loss=0.1309, Training Accuracy=0.9531\n",
      "iter: 57600, Minibatch Loss=0.2320, Training Accuracy=0.9453\n",
      "iter: 60800, Minibatch Loss=0.0717, Training Accuracy=0.9766\n",
      "iter: 64000, Minibatch Loss=0.1249, Training Accuracy=0.9453\n",
      "iter: 67200, Minibatch Loss=0.1085, Training Accuracy=0.9609\n",
      "iter: 70400, Minibatch Loss=0.1850, Training Accuracy=0.9609\n",
      "iter: 73600, Minibatch Loss=0.0951, Training Accuracy=0.9609\n",
      "iter: 76800, Minibatch Loss=0.1257, Training Accuracy=0.9688\n",
      "iter: 80000, Minibatch Loss=0.0766, Training Accuracy=0.9766\n",
      "iter: 83200, Minibatch Loss=0.1976, Training Accuracy=0.9453\n",
      "iter: 86400, Minibatch Loss=0.1044, Training Accuracy=0.9688\n",
      "iter: 89600, Minibatch Loss=0.0632, Training Accuracy=0.9844\n",
      "iter: 92800, Minibatch Loss=0.1025, Training Accuracy=0.9844\n",
      "iter: 96000, Minibatch Loss=0.1149, Training Accuracy=0.9453\n",
      "iter: 99200, Minibatch Loss=0.1645, Training Accuracy=0.9531\n",
      "Optimization Finished!\n",
      "Test Accuracy: 0.9922\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    step = 1\n",
    "    while step*batch_size < training_iters:\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        batch_x = batch_x.reshape((batch_size, n_steps, n_input))\n",
    "        feed_dict = { Xtr: batch_x, Ytr: batch_y }\n",
    "        sess.run(optimizer, feed_dict=feed_dict)\n",
    "        if step % display_step == 0:\n",
    "            cost_, accuracy_ = sess.run([cost, accuracy], feed_dict=feed_dict)\n",
    "            print (\"iter: %d, Minibatch Loss=%.4f, Training Accuracy=%.4f\"%\n",
    "                  (step*batch_size, cost_, accuracy_))\n",
    "        step += 1\n",
    "    print (\"Optimization Finished!\")\n",
    "    \n",
    "    test_len = 128\n",
    "    test_accuracy = sess.run(accuracy, feed_dict={\n",
    "        Xtr: mnist.test.images[:test_len].reshape((-1, n_steps, n_input)),\n",
    "        Ytr: mnist.test.labels[:test_len] })\n",
    "    print (\"Test Accuracy: %.4f\"% (test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
