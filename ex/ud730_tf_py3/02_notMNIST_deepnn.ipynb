{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set:  (200000, 28, 28) (200000,)\n",
      "Validateion set:  (10000, 28, 28) (10000,)\n",
      "Test set:  (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset = save['train_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    valid_dataset = save['valid_dataset']\n",
    "    valid_labels = save['valid_labels']\n",
    "    test_dataset  = save['test_dataset']\n",
    "    test_labels  = save['test_labels']\n",
    "    del save\n",
    "    print (\"Train set: \", train_dataset.shape, train_labels.shape)\n",
    "    print (\"Validateion set: \", valid_dataset.shape, valid_labels.shape)\n",
    "    print (\"Test set: \", test_dataset.shape, test_labels.shape)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set:  (200000, 28, 28) (200000, 10)\n",
      "Validateion set:  (10000, 28, 28) (10000, 10)\n",
      "Test set:  (10000, 28, 28) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(x, y):\n",
    "    x = x.reshape([-1, image_size, image_size]).astype(np.float)\n",
    "    y = (np.arange(num_labels) == y[:,None]).astype(np.float)\n",
    "    return x, y\n",
    "\n",
    "train_x, train_y = reformat(train_dataset, train_labels)\n",
    "valid_x, valid_y = reformat(valid_dataset, valid_labels)\n",
    "test_x,  test_y  = reformat(test_dataset,  test_labels)\n",
    "print (\"Train set: \", train_x.shape, train_y.shape)\n",
    "print (\"Validateion set: \", valid_x.shape, valid_y.shape)\n",
    "print (\"Test set: \", test_x.shape, test_y.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def bias_variable(shape):\n",
    "    initial = tf.zeros(shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, \n",
    "        strides=[1, 1, 1, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], \n",
    "        strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():        \n",
    "    \n",
    "    keep_prob = tf.placeholder( tf.float32 )\n",
    "    tf_train_x = tf.placeholder( tf.float32, shape=[None, image_size, image_size])\n",
    "    tf_train_y = tf.placeholder( tf.float32, shape=[None, num_labels])\n",
    "    tf_train_image = tf.reshape( tf_train_x, [-1, image_size, image_size, 1])\n",
    "    \n",
    "    w1 = weight_variable([5, 5, 1, 32])\n",
    "    b1 = bias_variable([32])\n",
    "    h1_relu = tf.nn.relu( conv2d(tf_train_image, w1) + b1)\n",
    "    h1 = max_pool_2x2( h1_relu )\n",
    "    \n",
    "    w2 = weight_variable([5, 5, 32, 64])\n",
    "    b2 = bias_variable([64])\n",
    "    h2_relu = tf.nn.relu( conv2d(h1, w2) + b2)\n",
    "    h2 = max_pool_2x2( h2_relu )\n",
    "    \n",
    "    flat_size = int((image_size/4)*(image_size/4)*64)\n",
    "    h2_flat = tf.reshape(h2, [-1, flat_size])\n",
    "    \n",
    "    w1_fc = weight_variable([flat_size, 1024])\n",
    "    b1_fc = bias_variable([1024])\n",
    "    h1_fc = tf.nn.dropout(\n",
    "        tf.nn.relu( tf.matmul(h2_flat, w1_fc) + b1_fc ), keep_prob )\n",
    "    \n",
    "    w2_fc = weight_variable([1024, 10])\n",
    "    b2_fc = bias_variable([10])    \n",
    "    \n",
    "    logits = tf.matmul(h1_fc, w2_fc) + b2_fc\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_y, logits=logits))\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "    \n",
    "    correct_pred = tf.equal( tf.argmax(logits, 1), tf.argmax(tf_train_y, 1) )\n",
    "    accuracy = tf.reduce_mean ( tf.cast(correct_pred, tf.float32) )            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "step 0, train accuracy 0.1 \n",
      "step 100, train accuracy 0.78 \n",
      "step 200, train accuracy 0.79 \n",
      "step 300, train accuracy 0.85 \n",
      "step 400, train accuracy 0.7 \n",
      "step 500, train accuracy 0.86 \n",
      "step 600, train accuracy 0.86 \n",
      "step 700, train accuracy 0.85 \n",
      "step 800, train accuracy 0.86 \n",
      "step 900, train accuracy 0.8 \n",
      "step 1000, train accuracy 0.88 \n",
      "step 1100, train accuracy 0.8 \n",
      "step 1200, train accuracy 0.85 \n",
      "step 1300, train accuracy 0.86 \n",
      "step 1400, train accuracy 0.85 \n",
      "step 1500, train accuracy 0.87 \n",
      "step 1600, train accuracy 0.86 \n",
      "step 1700, train accuracy 0.91 \n",
      "step 1800, train accuracy 0.87 \n",
      "step 1900, train accuracy 0.92 \n",
      "step 2000, train accuracy 0.82 \n",
      "step 2100, train accuracy 0.91 \n",
      "step 2200, train accuracy 0.88 \n",
      "step 2300, train accuracy 0.9 \n"
     ]
    }
   ],
   "source": [
    "train_size = train_dataset.shape[0]\n",
    "num_steps = 5001\n",
    "batch_size = 100\n",
    "num_batch = int(train_size/batch_size)\n",
    "offset = 0\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print (\"Initialized\")\n",
    "    for i in range(num_steps):\n",
    "        batch_x = train_x[offset:offset+batch_size,:]\n",
    "        batch_y = train_y[offset:offset+batch_size,:]\n",
    "        offset += batch_size\n",
    "        if ( offset >= train_size ): offset = 0\n",
    "        if i % 100 == 0:\n",
    "            train_accuracy = accuracy.eval( feed_dict = {\n",
    "                tf_train_x: batch_x, tf_train_y: batch_y, keep_prob: 1.0 })\n",
    "            print ('step %d, train accuracy %g ' % \n",
    "                   (i, train_accuracy))               \n",
    "#            valid_accuracy = accuracy.eval( feed_dict = {\n",
    "#                tf_train_x: valid_x, tf_train_y: valid_y, keep_prob: 1.0 })            \n",
    "#            print ('step %d, train accuracy %g, valid accuracy %g' % \n",
    "#                   (i, train_accuracy, valid_accuracy))       \n",
    "        optimizer.run( feed_dict = {\n",
    "            tf_train_x: batch_x, tf_train_y: batch_y, keep_prob: 0.5 })\n",
    "    print ('valid accuracy %g' % accuracy.eval(feed_dict = {\n",
    "        tf_train_x: valid_x, tf_train_y: valid_y, keep_prob: 1.0}))    \n",
    "    print ('test accuracy %g' % accuracy.eval(feed_dict = {\n",
    "        tf_train_x: test_x, tf_train_y: test_y, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
